"""
VibeVoice í•œê¸€ TTS ê°„ë‹¨ CLI ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
ì‚¬ìš©ë²•: python simple_tts.py --txt_path <í…ìŠ¤íŠ¸íŒŒì¼ê²½ë¡œ> --output <ì €ì¥ê²½ë¡œ>
"""
import os
import argparse
import time
import torch
import copy
from vibevoice.modular.modeling_vibevoice_streaming_inference import VibeVoiceStreamingForConditionalGenerationInference
from vibevoice.processor.vibevoice_streaming_processor import VibeVoiceStreamingProcessor

def main():
    parser = argparse.ArgumentParser(description="VibeVoice í•œê¸€ TTS CLI")
    parser.add_argument("--txt_path", type=str, required=True, help="ì…ë ¥ í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--output", type=str, default="output.wav", help="ì €ì¥í•  ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--speaker_name", type=str, default="Carter", help="í™”ì ì´ë¦„ (VibeVoice/demo/voices/streaming_model/ ë‚´ íŒŒì¼ ë§¤ì¹­)")
    parser.add_argument("--cfg_scale", type=float, default=1.5, help="CFG Scale (1.0 ~ 3.0)")
    args = parser.parse_args()

    # 1. í…ìŠ¤íŠ¸ ì½ê¸°
    if not os.path.exists(args.txt_path):
        print(f"âŒ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {args.txt_path}")
        return

    with open(args.txt_path, 'r', encoding='utf-8') as f:
        text = f.read().strip()
    
    if not text:
        print("âŒ í…ìŠ¤íŠ¸ íŒŒì¼ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        return

    print(f"ğŸ“ ì…ë ¥ í…ìŠ¤íŠ¸: {text[:50]}..." if len(text) > 50 else f"ğŸ“ ì…ë ¥ í…ìŠ¤íŠ¸: {text}")

    # 2. ëª¨ë¸ ë¡œë“œ
    print("ğŸ“¦ ëª¨ë¸ ë¡œë”© ì¤‘...")
    model_path = 'microsoft/VibeVoice-Realtime-0.5B'
    
    # Device ì„¤ì •
    if torch.cuda.is_available():
        device = 'cuda'
        load_dtype = torch.bfloat16
        attn_impl = 'flash_attention_2'
    elif torch.backends.mps.is_available():
        device = 'mps'
        load_dtype = torch.float32
        attn_impl = 'sdpa'
    else:
        device = 'cpu'
        load_dtype = torch.float32
        attn_impl = 'sdpa'

    print(f"ğŸ–¥ï¸  ë””ë°”ì´ìŠ¤: {device}")

    processor = VibeVoiceStreamingProcessor.from_pretrained(model_path)
    
    try:
        if device == 'cuda':
            model = VibeVoiceStreamingForConditionalGenerationInference.from_pretrained(
                model_path, torch_dtype=load_dtype, device_map='cuda', attn_implementation=attn_impl
            )
        elif device == 'mps':
            model = VibeVoiceStreamingForConditionalGenerationInference.from_pretrained(
                model_path, torch_dtype=load_dtype, attn_implementation=attn_impl, device_map=None
            )
            model.to('mps')
        else:
            model = VibeVoiceStreamingForConditionalGenerationInference.from_pretrained(
                model_path, torch_dtype=load_dtype, device_map='cpu', attn_implementation=attn_impl
            )
    except:
        # Fallback
        model = VibeVoiceStreamingForConditionalGenerationInference.from_pretrained(
            model_path, torch_dtype=load_dtype, 
            device_map=(device if device in ('cuda', 'cpu') else None), 
            attn_implementation='sdpa'
        )
        if device == 'mps':
            model.to('mps')

    model.eval()
    model.set_ddpm_inference_steps(num_steps=5)

    # 3. í™”ì íŒŒì¼ ì°¾ê¸°
    possible_voice_dirs = [
        os.path.join('VibeVoice', 'demo', 'voices', 'streaming_model'),
        os.path.join(os.path.expanduser('~'), 'VibeVoice', 'demo', 'voices', 'streaming_model'),
    ]
    
    voice_file = None
    for voice_dir in possible_voice_dirs:
        if os.path.exists(voice_dir):
            for filename in os.listdir(voice_dir):
                if filename.endswith('.pt') and args.speaker_name.lower() in filename.lower():
                    voice_file = os.path.join(voice_dir, filename)
                    break
            if voice_file:
                break
    
    if not voice_file:
        print(f"âŒ '{args.speaker_name}' í™”ì íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"ğŸ¤ í™”ì: {args.speaker_name}")
    
    target_device = device if device != 'cpu' else 'cpu'
    all_prefilled_outputs = torch.load(voice_file, map_location=target_device, weights_only=False)

    # 4. ì…ë ¥ ì²˜ë¦¬
    text = text.replace("'", "'").replace('"', '"').replace('"', '"')
    inputs = processor.process_input_with_cached_prompt(
        text=text, cached_prompt=all_prefilled_outputs,
        padding=True, return_tensors='pt', return_attention_mask=True
    )

    for k, v in inputs.items():
        if torch.is_tensor(v):
            inputs[k] = v.to(target_device)

    # 5. ìƒì„±
    print("ğŸµ ìŒì„± ìƒì„± ì¤‘...")
    start_time = time.time()
    
    outputs = model.generate(
        **inputs, max_new_tokens=None, cfg_scale=args.cfg_scale,
        tokenizer=processor.tokenizer, generation_config={'do_sample': False},
        verbose=False,
        all_prefilled_outputs=copy.deepcopy(all_prefilled_outputs)
    )
    
    generation_time = time.time() - start_time
    
    # 6. ì €ì¥
    if outputs.speech_outputs and outputs.speech_outputs[0] is not None:
        processor.save_audio(outputs.speech_outputs[0], output_path=args.output)
        print(f"âœ… ì €ì¥ ì™„ë£Œ: {args.output}")
        print(f"â±ï¸  ì†Œìš” ì‹œê°„: {generation_time:.2f}ì´ˆ")
    else:
        print("âŒ ìƒì„± ì‹¤íŒ¨")

if __name__ == "__main__":
    main()
